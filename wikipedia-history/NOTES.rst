==================
Wiki History Notes
==================

data was downloaded from urls.txt files are in files.txt

data is distributed over 6 hosts, st3-8 based on the hostfiles.txt
file which is generated by hosts.py

This folder is derived from st3.p.fir.io:sandbox/crate-wikipedia which
is distributed over above mentioned hosts. However future imports of
this data should update this git repo and do a checkout on the hosts.

====================
JSON data generation
====================

converted via run.sh (see source) in screen on every host concurrent
takes about 6 hours

xml is bigger because there is no text in json

input files:

$ dsh -g cr6 'du -hs /mnt/data20/wikipedia_history/'
st3.p.fir.io: 68G	/mnt/data20/wikipedia_history/
st4.p.fir.io: 90G	/mnt/data20/wikipedia_history/
st5.p.fir.io: 90G	/mnt/data20/wikipedia_history/
st6.p.fir.io: 114G	/mnt/data20/wikipedia_history/
st7.p.fir.io: 104G	/mnt/data20/wikipedia_history/
st8.p.fir.io: 71G	/mnt/data20/wikipedia_history/

output files:

st3.p.fir.io: 19G	/mnt/data21/wikipedia_history/
st4.p.fir.io: 20G	/mnt/data21/wikipedia_history/
st5.p.fir.io: 20G	/mnt/data21/wikipedia_history/
st6.p.fir.io: 28G	/mnt/data21/wikipedia_history/
st7.p.fir.io: 17G	/mnt/data21/wikipedia_history/
st8.p.fir.io: 18G	/mnt/data21/wikipedia_history/

Import
======

create indexes:
 for idx in $(cut -d ' ' -f 2 hostfiles.txt | cut -d '.' -f 1 |sort|uniq); do ./create_index.sh $idx; done


import with import.sh > import.log in screen

speed:

with 2 concurrent loads
by counting like this
watch -n 10 curl -sS 'http://st1.p.fir.io:9200/*history*/_count?pretty'
got about 140k-180k inserts per sec

with 3 concurrent loads  140k- 200k are possible

Aliases
=======

created aliase
for idx in $(cut -d ' ' -f 2 hostfiles.txt | cut -d '.' -f 1 |sort|uniq); do  curl -sS -XPUT 'http://st1.p.fir.io:9200/'idx'/_alias/enwiki-history?pretty' ; done



